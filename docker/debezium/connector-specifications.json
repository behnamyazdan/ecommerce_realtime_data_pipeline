{
  "name": "my-postgres-connector",
  "config": {
    // Connector class
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    // Number of tasks
    "tasks.max": "1",

    // PostgreSQL server details
    "database.hostname": "your_postgres_host",
    "database.port": "5432",
    "database.user": "your_username",
    "database.password": "your_password",
    "database.dbname": "your_database",

    // Unique identifier for the connector
    "database.server.name": "postgres-server",

    // Specify how to handle decimal values
    "decimal.handling.mode": "precise",

    // Specify the capture instance name
    "name": "my-postgres-connector",

    // Specify the slot name for logical replication
    "slot.name": "debezium_slot",

    // Snapshot mode: initial (full snapshot) or never (no snapshot)
    "snapshot.mode": "initial",

    // Controls the output of schema changes
    "include.schema.changes": "true",

    // Specify the replication slot options
    "slot.drop_on_stop": "false",

    // Specify the maximum number of records to include in a batch
    "max.batch.size": "2048",

    // Specify the maximum number of milliseconds to wait for new data to arrive
    "poll.interval.ms": "100",

    // Specify the time to wait before trying to reconnect to the database after a failure
    "heartbeat.interval.ms": "2000",

    // Specify how often to refresh the connector metadata
    "snapshot.refresh.interval.ms": "1000",

    // Control whether tables that are no longer present in the database should be removed
    "snapshot.drop.on.stop": "false",

    // Specify the database history storage
    "database.history.kafka.bootstrap.servers": "kafka:9092",
    "database.history.kafka.topic": "dbhistory.my-postgres-connector",

    // Specify the maximum size of an individual message's output
    "max.queue.size": "8192",

    // Specify how often to perform a snapshot of the database
    "snapshot.minimal.locks": "true",

    // Specify the number of milliseconds to delay before canceling transactions during snapshot
    "snapshot.max.dml.events": "5000",

    // Control the execution of the Postgres "CREATE SCHEMA" statements for logical decoding
    "database.include.list": "schema1,schema2",

    // Specify the method to determine the location of a database table's primary key
    "table.key.primary.filtering": "true",

    "pk.mode": "string",

    // Specify the maximum number of tables for which schema history is tracked
    "database.history.store.only.monitored.tables.ddl": "false",

    // Specify whether to skip large initial changesets during snapshotting
    "snapshot.fetch.size.multiplier": "2",

    // Control the size of the binlog reader cache
    "snapshot.max.message.size": "4096",

    // Specify whether to include the primary key names in the captured events
    "snapshot.fetch.table.key.primary.key": "false",

    // Specify the maximum size of each individual batch of records sent to the Kafka topic
    "snapshot.fetch.table.key.default.name": "id",

    // Control whether to enable schema changes for columns with the same name but different types
    "snapshot.lock.timeout.ms": "5000",

    // Specify the frequency to validate the PostgreSQL logical replication slot
    "decoderbufs.lsn.min.message.bytes": "8192",

    // Specify the maximum delay before a heartbeat message is sent to the database server
    "decoderbufs.lsn.precision.mode": "micro",

    // Specify the list of schemas to be captured
    "table.include.list": "schema1.table1,schema2.table2",

    // Specify the timeout for the initial snapshot
    "table.exclude.list": "schema3.table3,schema4.table4",

    // Specify how long to wait for a response from the server
    "time.precision.mode": "adaptive",

    // Control whether tombstones are included for deleted records
    "tombstones.on.delete": "false",

    // Specify the maximum number of operations per transaction
    "max.transaction.size.bytes": "1048576",

    // Specify the method to fetch the WAL offsets
    "wal.keep.segment.hours": "10",

    // Specify the time interval to check for new WAL segments
    "wal.producer.buffer.ms": "2000",

    // Control the buffer size for changes stored in memory
    "wal.consumer.buffer.ms": "500",

    // Specify the initial delay before fetching changes from the server
    "heartbeat.interval.ms": "2000",

    // Specify the delay before fetching changes after a snapshot
    "heartbeat.action.query.ms": "20000",

    // Control how the connector reacts when it encounters an unparseable message
    "include.query": "false",

    // Specify the frequency of log messages related to the replication process
    "database.history.kafka.recovery.poll.interval.ms": "10000",

    // Specify the timeout for Kafka connections
    "database.history.skip.unparseable.ddl": "true",

    // Specify the topic prefix for schema changes
    "database.history.skip.failed.operations": "true",

    // Control whether the connector can skip failed DDL operations
    "snapshot.select.statement.overrides.by.table": "schema1.table1:SELECT * FROM schema1.table1 WHERE id > 100",


    // Specify the type of output message envelope to use
    "transforms": "unwrap",

    // Control how the connector reacts when encountering special characters
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",

    // Specify the behavior for handling records with special characters
    "transforms.unwrap.drop.tombstones": "false",

    // Control whether tombstone records are included in the output stream
    "transforms.unwrap.delete.handling.mode": "rewrite",

    // Specify the behavior for handling deleted records
    "transforms.unwrap.operation.header": "true",


    // Specify the behavior for handling deleted records
    "transforms.unwrap.add.fields": "db",


    // Specify additional source fields to include in the output stream
    "transforms.unwrap.add.headers": "true",

    // Control whether headers are included in the output stream
    "transforms.unwrap.add.headers.field": "headers",

    // Specify the field to store additional headers
    "transforms.unwrap.add.source.fields": "source.ts_ms",

    // Specify additional source fields to include in the output stream
    "transforms.unwrap.add.schema.metadata": "true",

    // Control whether schema metadata is included in the output stream
    "transforms.unwrap.add.schema.metadata.field": "schema",

    // Specify the SSL mode for the database connection
    "database.sslmode": "require",

    // Specify the TCP keep-alive setting for the database connection
    "database.keepAlive": "true",

    // Specify the optional JDBC connection properties
    "database.connect.timeout": "30",
    "database.socketTimeout": "30",
    "database.fetchSize": "1000",
    "database.maximum.pool.size": "5",
    "database.minimum.idle": "1",

    // Specify the host for logical replication slots
    "slot.host": "localhost",

    // Specify the optional message transformation settings,
    "transforms.unwrap.addTopicPrefix.prefix": "dbserver1-",

    // Specify the maximum number of attempts for retrying a failed schema change statement
    "schema.change.policy.max.attempts": "10",

    // Specify the timeout for retrying a failed schema change statement
    "schema.change.policy.timeout.ms": "5000",

    // Specify the policy for handling failed schema changes
    "schema.change.policy.behavior.on.error": "retry",



  }
}
